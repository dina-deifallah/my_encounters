{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward and backprop from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"feedforward.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=50, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Add a bias (parameter b) column to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    '???'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = add_bias(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: calculate the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '???'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
    "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
    "assert np.all(sigmoid(a).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights):\n",
    "\n",
    "    \"\"\"\n",
    "    1. Multiply the input matrix X\n",
    "       with the weights of the first layer.\n",
    "\n",
    "    2. Apply the sigmoid function on the result.\n",
    "\n",
    "    3. Append an extra column of ones to the result (i.e. the bias).\n",
    "\n",
    "    4. Multiply the output of the previous step\n",
    "       with the weights of the second (i.e. outer) layer.\n",
    "\n",
    "    5. Apply the sigmoid function on the result.\n",
    "\n",
    "    6. Return all intermediate results (i.e. anything that is outputted\n",
    "       by an activation function).\n",
    "    \"\"\"\n",
    "\n",
    "    return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2 = feed_forward(X, w)\n",
    "assert out1.shape == (50, 2)\n",
    "assert out2.shape == (50, 1)\n",
    "\n",
    "Xref = np.array([[1.0, 2.0, 1.0]])\n",
    "whidden = np.array([[1.0, 2.0, 0.0],\n",
    "                 [-1.0, -2.0, 0.0]\n",
    "                    ]).T\n",
    "wout = np.array([[1.0, -1.0, 0.5]]).T\n",
    "\n",
    "out1, out2 = feed_forward(Xref, [whidden, wout])\n",
    "assert np.all(out1.round(2) == np.array([[0.99, 0.01]]))\n",
    "assert np.all(out2.round(2) == np.array([[0.82]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"neuron_w_backprop.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's talk about loss function!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we'll use log loss as a loss function:\n",
    "$$ loss = -(y_{true} log(y_{pred}) + (1-y_{true}) log(1-y_{pred})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Run feed-forward and make sure it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Write a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a: Log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = -(y_{true} log(y_{pred}) + (1-y_{true}) log(1-y_{pred})) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(ytrue, ypred):\n",
    "    loss = ___ \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "ypred = np.array([0.01, 0.99, 0.01, 0.99])\n",
    "expected = np.array([0.01, 4.61, 4.61, 0.01])\n",
    "assert np.all(log_loss(ytrue, ypred).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b: Log-loss derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Formula_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_deriv(ytrue, ypred):\n",
    "    loss_deriv = ___  \n",
    "    return loss_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.5, 0.3, 0.99, 0.2])\n",
    "b = np.array([0.4, 0.2, 0.10, 0.3])\n",
    "expected = np.array([-0.42, -0.62, -9.89, 0.48])\n",
    "assert np.all(log_loss_deriv(a, b).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra — 2c: Sigmoid derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Formula_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(X):\n",
    "    return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate initial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2 = feed_forward(X, w)\n",
    "ytrue = y.reshape(-1, 1)\n",
    "log_loss(___, ___) #which arrays do we need to compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(ytrue, out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write a backpropagation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks of the following function, which transcribes the equations from earlier (equations A - E) to run one iteration of the backpropagation algorithm. It takes in a handful of arguments:\n",
    "* the initial weights,\n",
    "* the outputs from the feed-forward process (i.e. both the hidden output and the final output),\n",
    "* the true labels,\n",
    "* the input data,\n",
    "* and the learning rates (we’ll have a separate learning rate for each layer of the network).\n",
    "\n",
    "The function (representing a single iteration of the backpropagation algorithm), should return the modified hidden weights and the modified outer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(weights,\n",
    "             output1,\n",
    "             output2,\n",
    "             ytrue,\n",
    "             X_input,\n",
    "             LR):\n",
    "\n",
    "    wH = weights[0]\n",
    "    wO = weights[1]\n",
    "\n",
    "    '''EQUATION A:'''\n",
    "    error = log_loss_deriv(___ , ___)\n",
    "\n",
    "    '''EQUATION B:'''\n",
    "    # don't forget the bias!\n",
    "    hidden_out_with_bias = add_bias(___)\n",
    "    # derivative of the sigmoid function with respect to the\n",
    "    # hidden output * weights\n",
    "    sig_deriv_1 = sigmoid_deriv( ___ )\n",
    "\n",
    "    y_grad = sig_deriv_1 * error\n",
    "\n",
    "    '''EQUATION C:'''\n",
    "    delta_wo = -np.dot( ___.T, hidden_out_with_bias ) * LR\n",
    "\n",
    "    #and finally, old weights + delta weights -> new weights!\n",
    "    wO_new = wO + ___.T\n",
    "\n",
    "    '''EQUATION D:'''\n",
    "    sig_deriv_2 = sigmoid_deriv( ___ )\n",
    "    #exclude the bias (last column) of the outer weights,\n",
    "    #since it is not backpropagated!\n",
    "    H_grad = ___  * np.dot(y_grad , ___[:-1].T)\n",
    "\n",
    "    '''EQUATION E:'''\n",
    "    delta_wH = -np.___(H_grad.T, ___ ) * ___\n",
    "    #old weights + delta weights -> new weights!\n",
    "    wH_new = wH + ___.T\n",
    "\n",
    "    # new hidden weights, new output weights\n",
    "    return ___, ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Run the Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your backpropagation algorithm in a loop! Inside the loop:\n",
    "* Run your feed-forward function with the X data and the starting weights (which are initially random!).\n",
    "* Collect the total sum of the log-loss values into a list, so we can track them over time.\n",
    "* Run your backprop function to get the modified weights.\n",
    "* At the end of the loop, make your modified weights the new weights for the next cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X = add_bias(X)\n",
    "y = y.reshape(-1, 1)\n",
    "weights = [\n",
    "   np.random.normal(size=(3, 2)),\n",
    "   np.random.normal(size=(3, 1))\n",
    "]\n",
    "\n",
    "# train\n",
    "LOSS_VEC = []\n",
    "\n",
    "for i in range(1000):\n",
    "    out1, out2 = feed_forward(X, weights)\n",
    "    LOSS_VEC.append(___.sum())\n",
    "    new_weights = backprop(___, ___, ___, ___, ___, ___)\n",
    "    weights = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6a: Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LOSS_VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6b: Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of values\n",
    "x = np.linspace(-3, 3, 200)\n",
    "X_vis = np.array([(x1, x2) for x1 in x for x2 in x])\n",
    "# add the bias column\n",
    "X_vis = add_bias(X_vis)\n",
    "\n",
    "# calculate the (random) predictions\n",
    "_, y_pred = feed_forward(X_vis, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the predictions for visualization\n",
    "Z = y_pred.reshape((len(x), len(x)), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a contour plot\n",
    "fig,ax=plt.subplots(1,1)\n",
    "cp = ax.contourf(x, x, Z, alpha=0.8, cmap='coolwarm')\n",
    "ax.contour(x, x, Z, levels=[0.5])\n",
    "fig.colorbar(cp) # Add a colorbar to a plot\n",
    "\n",
    "# draw the original data\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
